





"""dataframe module"""
import pandas as pd
import numpy as np

"""ipynb module"""
from IPython.display import display, HTML

import os
import matplotlib.pyplot as plt
import seaborn as sns





RAW_DATA_PATH = '../../data/raw_data/'

CLEANED_DATA_PATH = '../../data/clean_data/'

PROCESSED_DATA_PATH = '../../data/pre_processed-data/'

LOCATION_FILE_NAME = ['SonLa','LangSon', 'HaNoi',
                    'NgheAn', 'DaNang', 'LamDong',
                    'HoChiMinh','BenTre']

LOCATION_VALUES = ['Sơn La','Lạng Sơn', 'Hà Nội',
                    'Nghệ An', 'Đà Nẵng', 'Lâm Đồng',
                    'Hồ Chí Minh','Bến Tre']

SAVE_COLS = [ 'Address', 'Datetime', 'DatetimeEpoch',
            'Tempmax', 'Tempmin', 'Temp', 'Dew', 
            'Humidity', 'Precip', 'Precipprob', 'Precipcover',
            'Preciptype', 'Windgust', 'Windspeed', 'Winddir', 
            'Pressure', 'Cloudcover', 'Visibility', 'Solarradiation', 
            'Solarenergy', 'Uvindex', 'Moonphase' ]

NON_STATISTICS_STATS = ['DatetimeEpoch']

STATISTICS_NAMES = ['Count', 'Mean', 'Standard Deviation',
                    'Min', 'Lower Quartile', 'Median',
                    'Upper Quartile', 'Max', 'Missing Ratio',
                    'Range', 'Variance']

ORDERED_STATISTICS = ['Count','Missing Ratio', 'Min', 'Max', 'Mean',
                    'Median', 'Lower Quartile', 'Upper Quartile',
                    'Range',  'Standard Deviation', 'Variance']





def read_raw_datas() -> list:
    """Takes a location file name and return 
    the pandas dataframe from that csv """
    result_list = []

    for file_name in LOCATION_FILE_NAME:
        df = pd.read_csv(RAW_DATA_PATH + f'{file_name}.csv')
        result_list.append(df)

    return result_list


def read_cleaned_datas() -> list:
    """Takes a location file name and return 
    the pandas dataframe from that csv """
    result_list = []

    for file_name in LOCATION_FILE_NAME:
        df = pd.read_csv(CLEANED_DATA_PATH + f'{file_name}.csv')
        result_list.append(df)

    return result_list


def add_address_column(df, location_name) -> pd.DataFrame:
    """Drop old address columns and add standardized one"""
    if 'Address' in df.columns:
        df.drop('Address', axis=1)
    if 'address' in df.columns:
        df.drop('address', axis=1)

    address_list = [location_name for _ in range(len(df.index))]
    df = df.assign(Address = pd.Series(address_list))
    return df


def drop_unnecessary_columns(df) -> pd.DataFrame:
    """Drop unnecessary features to clean up datas,
    also rearrange columns with a fixed order"""
    df = df[SAVE_COLS]
    return df


def upper_case_name(df) -> pd.DataFrame:
    """Make all columns name uppercase"""
    col_names = df.columns
    upper_col_name = [col_name[0].upper() + col_name[1:]
                      for col_name in col_names]

    df.rename({col_names[i] : upper_col_name[i]
               for i in range(len(col_names))},
              axis=1, inplace=True)

    return df


def export_cleaned_df(df, location_name):
    """export cleaned up dataframe to csv files"""
    df.to_csv(CLEANED_DATA_PATH + f'{location_name}.csv', index=False)


"""Clean function"""
dataframe_list = read_raw_datas()
for i, df in enumerate(dataframe_list):
    df = add_address_column(df, LOCATION_VALUES[i])
    df = upper_case_name(df)
    df = drop_unnecessary_columns(df)
    export_cleaned_df(df, LOCATION_FILE_NAME[i])











def is_duplicated(df) -> bool:
    """Check if a dataframe have duplicated row"""
    dup_count = df.duplicated().sum()
    return dup_count 


def check_duplicated():
    """check if any file have duplicated datas"""
    dataframe_list = read_cleaned_datas()
    is_dup = False
    for country_df in dataframe_list: 
        num_duplicated_rows = 0
        num_duplicated_rows += is_duplicated(country_df)

        if num_duplicated_rows:
            print(f"Your raw data {country_df['Address'][0]}.csv have {num_duplicated_rows}",
                "duplicated data. Please de-deduplicate your raw data.!")
            is_dup = True
    if not is_dup:
        print("Your data doesn't have duplicated data")




check_duplicated()





raw_df = pd.read_csv(CLEANED_DATA_PATH +'DaNang.csv')
shape = raw_df.shape
print('Current shape (1 location): ', shape)





raw_df.info()








def missing_ratio(arr):
    ratio = arr.isna().mean()
    return ratio


def check_missing():
    for location in LOCATION_FILE_NAME:
        df = pd.read_csv(CLEANED_DATA_PATH + f'{location}.csv', index_col=0)
        tmp = df.agg([missing_ratio])
        filtered = [h for h in tmp.columns if tmp[h].iloc[0] > 0]
        print(location)
        print(tmp[filtered])


check_missing()





def fill_median(x):
    return x.fillna(x.median())


# Chứa dataframe đã được xử lý dữ liệu của các tỉnh thành 
processed_df = []

def fill_missing():
    for location in LOCATION_FILE_NAME:
        df = pd.read_csv(CLEANED_DATA_PATH + f'{location}.csv')
        df['Visibility'] = df.groupby('Address')['Visibility'].transform(fill_median)
        tmp = df.agg([missing_ratio])
        filtered = [h for h in tmp.columns if tmp[h].iloc[0] > 0]
        print(location)
        print(tmp[filtered])
        processed_df.append((location, df))


fill_missing() # fill missing values và kiểm tra kết quả





def export_processed_df(df, location_name):
    """export processed dataframe to csv files"""
    df.to_csv(PROCESSED_DATA_PATH + f'{location_name}.csv', index=False)

def save_processed_df(processed_dataframes):
    for location, df in processed_dataframes:
        export_processed_df(df, location)


save_processed_df(processed_df)








def standardize_statistics(df) -> pd.DataFrame:
    """Change column's names to standard name 
    and re-order all the columns"""

    old_name = df.columns.tolist()
    df.rename({old_name[i] : STATISTICS_NAMES[i]
               for i in range(len(df.columns))},
               axis = 1, inplace = True)

    df = df[ORDERED_STATISTICS]
    return df 





def Statistics (df) -> pd.DataFrame: 

    stats = [x for x in df.columns.to_list() if x not in NON_STATISTICS_STATS]
    result_df = df[stats].describe().round(2)
    # update columns name 
    stats = result_df.columns.to_list()
    # print(stats)

    missing_list = []
    range_list = []
    variance = []

    for col in stats: 
        missing_list.append(missing_ratio(df[col]))
        range_list.append(result_df[col]['max'] - result_df[col]['min'])
        variance.append(df[col].var())

    result_df.loc['Missing ratio'] = missing_list
    result_df.loc['Range'] = range_list
    result_df.loc['Variance'] = variance

    return result_df.T





for location in LOCATION_FILE_NAME:
    raw_df = pd.read_csv(CLEANED_DATA_PATH + location + '.csv', index_col=0)
    stat_df = Statistics(raw_df)
    stat_df = standardize_statistics(stat_df)
    print(location)
    display(stat_df)
    print()





def visualize_df(df):

    df_numeric_cols = df.select_dtypes(include='float')
    # descriptive_statistics = Statistics(df_numeric_cols).T
    
    # m_ratio = missing_ratio(df_numeric_cols).round(2)
    # descriptive_statistics.loc['missing_ratio'] = m_ratio
    # descriptive_statistics
    Q1 = df_numeric_cols.quantile(0.25)
    Q3 = df_numeric_cols.quantile(0.75)
    IQR = Q3 - Q1

    # Xác định ngưỡng để nhận diện ngoại lai
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Nhận diện ngoại lai: Dữ liệu nằm ngoài khoảng [lower_bound, upper_bound]
    outliers = (df_numeric_cols < lower_bound) | (df_numeric_cols > upper_bound)

    # Vẽ histogram với các ngoại lai được đánh dấu
    n_cols = len(df_numeric_cols.columns)
    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4 * n_cols))

    # Nếu chỉ có một cột, axes sẽ không phải là danh sách mà là một đối tượng đơn lẻ
    if n_cols == 1:
        axes = [axes]

    for i, col in enumerate(df_numeric_cols.columns):
        # Vẽ histogram với KDE
        sns.histplot(df_numeric_cols[col], bins=20, kde=True, ax=axes[i])
        
        # Lọc các ngoại lai cho cột hiện tại
        outliers_col = df_numeric_cols[col][outliers[col]]
        
        # Đánh dấu ngoại lai trên histogram bằng điểm đỏ
        axes[i].scatter(outliers_col, [0] * len(outliers_col), color='red', label='Outliers', zorder=5)
        
        # Cài đặt tiêu đề và nhãn
        axes[i].set_title(f'Histogram of {col} with Outliers')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Frequency')
        axes[i].legend()

    # Tinh chỉnh layout và hiển thị biểu đồ
    plt.tight_layout()
    plt.show()

    # Tạo báo cáo số lượng ngoại lai trong từng cột
    outliers_summary = outliers.sum()
    print("Số lượng ngoại lai trong từng cột:")
    print(outliers_summary)


def visualize_datas():
    dataframe_list = read_cleaned_datas()
    for i, df in enumerate(dataframe_list):
        print(f'Location: {LOCATION_VALUES[i]}')
        visualize_df(df)
        print()
        print()


visualize_datas()
